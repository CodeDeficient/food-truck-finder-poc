# Food Truck Finder - Work Breakdown Structure (WBS) & Roadmap

This document provides a detailed breakdown of all tasks required to improve and enhance the Food Truck Finder application. It serves as our central roadmap for development, tracking progress, and ensuring we adhere to project standards.

---

## 1.0 Project Management & Foundation

- **[ ] 1.1: Establish and Maintain WBS Roadmap**
  - **Guidance:** This task involves the continuous refinement of this WBS document. As we research and complete tasks, we will update the status, add new sub-tasks, and adjust priorities. This document is our single source of truth for the project plan.
  - **CCR:** C:2, C:10, R:1
  - **Verification:** The WBS is regularly updated after each major task completion.

- **[x] 1.2: Review Existing Project Documentation**
  - **Guidance:** Thoroughly read and understand the contents of `supabase_advisor.md` and `supabase_improvement_plan.md` to identify immediate, actionable items recommended by Supabase.
  - **CCR:** C:3, C:9, R:2
  - **Verification:** A summary of key findings from these documents is created and integrated into the relevant sections of this WBS.

- **[ ] 1.3: Enforce Zero-Trust Verification Protocol**
  - **Guidance:** For all subsequent tasks, strictly adhere to the "Zero-Trust Post-Action Verification Protocol." After every 10-15 significant edits, run the holy trinity of checks: `npx tsc --noEmit`, `npx eslint .`, and `npx jscpd .` to ensure we are not introducing new errors.
  - **CCR:** C:3, C:10, R:3
  - **Verification:** Command history and task outputs demonstrate regular execution of these verification checks.

---

## 2.0 Security & Access Control

- **[ ] 2.1: Secure Admin Dashboard**
  - **Guidance:** The `/admin` route and all its sub-routes must be protected. Currently, it's publicly accessible. The goal is to restrict access to a single, authenticated developer user (me).
  - **CCR:** C:7, C:8, R:8
  - **Verification:** Unauthorized users navigating to `/admin` are redirected to a login page or an access denied page. The designated admin user can log in and access the dashboard.
  - **[x] 2.1.1: Research Authentication Provider**
    - **Guidance:** Per user direction, the primary authentication provider will be Google, managed through a service like Firebase Authentication to decouple it from Supabase. Research the integration of Firebase Auth with a Supabase backend. The goal is to use Firebase for sign-in and then pass the user's identity (JWT) to Supabase to control data access via RLS.
    - **CCR:** C:7, C:7, R:7
    - **Verification:** A new document, `docs/AUTH_ARCHITECTURE.md`, is created detailing the chosen implementation strategy.
  - **[x] 2.1.2: Research Role-Based Access Control (RBAC) with External JWT**
    - **Guidance:** Investigate best practices for implementing RBAC in Supabase using a JWT from an external provider (like Firebase). This involves setting up a custom `auth.users` table or a separate `profiles` table with a `role` column, and creating RLS policies that check this role.
    - **CCR:** C:6, C:7, R:6
    - **Verification:** A clear plan for managing admin roles is added to `docs/AUTH_ARCHITECTURE.md`.
  - **[ ] 2.1.3: Implement Supabase RBAC Schema**
    - **Guidance:** Create the necessary SQL types and tables in Supabase to support a permission-based RBAC system, as detailed in `docs/AUTH_ARCHITECTURE.md`.
    - **CCR:** C:4, C:9, R:4
    - **Verification:** The `app_role` and `app_permission` types, and the `role_permissions` table are successfully created in the Supabase database.
  - **[ ] 2.1.4: Implement Supabase `authorize` Function**
    - **Guidance:** Create the `public.authorize` SQL function in Supabase. This function will be the central point for checking user permissions in all RLS policies.
    - **CCR:** C:5, C:8, R:6
    - **Verification:** The `authorize` function is created and returns the expected boolean values when tested with different roles and permissions in the Supabase SQL Editor.
    - **[ ] 2.1.4.1: Draft `authorize` Function SQL**
      - **Guidance:** Write the initial SQL for the function based on the architecture defined in `docs/AUTH_ARCHITECTURE.md`.
      - **CCR:** C:3, C:9, R:3
      - **Verification:** The SQL code is written and passes a syntax check.
    - **[ ] 2.1.4.2: Test `authorize` Function with Mock JWTs**
      - **Guidance:** In the Supabase SQL Editor, use the `set_config` command to simulate JWTs with different roles (`admin`, `authenticated`) and test the function's output against various permissions.
      - **CCR:** C:4, C:9, R:4
      - **Verification:** The function returns `true` for authorized roles and `false` for unauthorized roles.
  - **[ ] 2.1.5: Refactor RLS Policies to Use `authorize` Function**
    - **Guidance:** Update all existing RLS policies on tables like `trucks`, `events`, etc., to use the new `authorize` function instead of direct role checks.
    - **CCR:** C:6, C:7, R:7
    - **Verification:** RLS policies are updated, and data access rules are correctly enforced for different user roles.
    - **[ ] 2.1.5.1: Refactor `trucks` Table RLS**
      - **Guidance:** Update the RLS policies for the `trucks` table.
      - **CCR:** C:3, C:9, R:4
      - **Verification:** The policies are updated and tested.
    - **[ ] 2.1.5.2: Refactor `events` Table RLS**
      - **Guidance:** Update the RLS policies for the `events` table.
      - **CCR:** C:3, C:9, R:4
      - **Verification:** The policies are updated and tested.
    - **[ ] 2.1.5.3: Refactor `menu_items` Table RLS**
      - **Guidance:** Update the RLS policies for the `menu_items` table.
      - **CCR:** C:3, C:9, R:4
      - **Verification:** The policies are updated and tested.
  - **[ ] 2.1.6: Configure Firebase and Supabase Integration**
    - **Guidance:** Follow the steps in `docs/AUTH_ARCHITECTURE.md` to add the Firebase Project ID as a trusted JWT issuer in the Supabase dashboard.
    - **CCR:** C:3, C:9, R:4
    - **Verification:** The integration is successfully created in the Supabase dashboard.
  - **[ ] 2.1.7: Implement Firebase Cloud Function for Custom Claims**
    - **Guidance:** Deploy a `beforeUserCreated` blocking function in Firebase to assign a default `authenticated` role to new users.
    - **CCR:** C:6, C:7, R:6
    - **Verification:** New users created via Firebase Auth have the `role: 'authenticated'` custom claim in their JWT.
    - **[ ] 2.1.7.1: Write Cloud Function Code**
      - **Guidance:** Write the Node.js code for the `beforeUserCreated` function.
      - **CCR:** C:4, C:8, R:4
      - **Verification:** The code is written and passes linting.
    - **[ ] 2.1.7.2: Deploy and Test Cloud Function**
      - **Guidance:** Deploy the function to Firebase and test the user creation flow.
      - **CCR:** C:4, C:8, R:5
      - **Verification:** A new test user is created, and their JWT is inspected to confirm the custom claim.
  - **[ ] 2.1.8: Manually Assign Admin Role in Firebase**
    - **Guidance:** Using the Firebase Admin SDK in a secure, one-off script, assign the `admin` role to the designated admin user's Firebase UID.
    - **CCR:** C:3, C:9, R:5
    - **Verification:** The admin user's JWT contains the `role: 'admin'` custom claim.
  - **[ ] 2.1.9: Configure Next.js Supabase Client for Firebase JWT**
    - **Guidance:** Update the Supabase client initialization in the Next.js app to dynamically use the JWT from the authenticated Firebase user.
    - **CCR:** C:5, C:8, R:6
    - **Verification:** The Supabase client successfully authenticates requests using the Firebase JWT.
  - **[ ] 2.1.10: Implement Middleware for Route Protection**
    - **Guidance:** Create or modify `app/middleware.ts` to check for a user's session (via Firebase) and their `admin` role (from the JWT) before allowing access to `/admin/*` paths.
    - **CCR:** C:6, C:8, R:6
    - **Verification:** Middleware correctly intercepts and redirects non-admin users away from admin routes.

- **[ ] 2.2: Research and Plan for AI Red Teaming**
  - **Guidance:** Locate the GitHub repository for "AI Red Teaming with MCP" and research how to set it up. The goal is to proactively identify security vulnerabilities in the codebase, especially before implementing payment features.
  - **CCR:** C:8, C:5, R:7
  - **Verification:** A new document, `docs/SECURITY_PLAN.md`, is created, outlining the plan for red teaming, including setup instructions and a schedule for running scans.
    - **[ ] 2.2.1: Locate and Analyze Red Teaming Repository**
      - **Guidance:** Find the specified GitHub repository and analyze its README and documentation to understand its capabilities and setup process.
      - **CCR:** C:4, C:6, R:3
      - **Verification:** The repository is located and a summary of its features is added to `docs/SECURITY_PLAN.md`.
    - **[ ] 2.2.2: Document Setup and Configuration**
      - **Guidance:** Detail the step-by-step process for installing, configuring, and running the AI Red Teaming tool against our codebase.
      - **CCR:** C:4, C:6, R:4
      - **Verification:** The setup instructions are clearly documented in `docs/SECURITY_PLAN.md`.
    - **[ ] 2.2.3: Define Red Teaming Schedule and Scope**
      - **Guidance:** Outline a schedule for running the red teaming scans (e.g., before major releases, on a quarterly basis) and define the initial scope of the scans.
      - **CCR:** C:3, C:7, R:4
      - **Verification:** The schedule and scope are documented in `docs/SECURITY_PLAN.md`.

---

## 3.0 Core Data Pipeline & Quality

- **[ ] 3.1: Verify Vercel CRON Job**
  - **Guidance:** The automated scraping process is the lifeblood of this app. We need to confirm the Vercel CRON job that triggers the scraping is running correctly and on schedule.
  - **CCR:** C:6, C:7, R:8
  - **Verification:** Vercel logs show the CRON job executing successfully at the expected intervals. The "last scraped at" timestamps in the database are updated as expected.
    - **[ ] 3.1.5: Manually Trigger CRON Jobs for Testing**
      - **Guidance:** Instead of waiting for the scheduled time, use `curl` or a similar tool to send a POST request with the `CRON_SECRET` to the `/api/cron/auto-scrape` and `/api/cron/quality-check` endpoints to test them on demand.
      - **CCR:** C:3, C:8, R:4
      - **Verification:** The manual requests trigger the jobs, and the expected logs and database changes occur.
    - **[ ] 3.1.6: Monitor Production CRON Job Execution**
      - **Guidance:** After deploying the changes, monitor the Vercel logs for the first few scheduled runs at 8:00 AM EST to confirm they are executing automatically.
      - **CCR:** C:2, C:8, R:3
      - **Verification:** The jobs run successfully without manual intervention.
  - **[ ] 3.1.1: Research CRON Job Configuration**
    - **Guidance:** Review `vercel.json` to identify all configured CRON jobs, their paths, and their schedules.
    - **CCR:** C:1, C:10, R:0
    - **Verification:** All CRON jobs are identified and listed in the sub-tasks below.
  - **[ ] 3.1.2: Analyze `auto-scrape` Job**
    - **Guidance:** The CRON job at `/api/cron/auto-scrape` runs daily at 8:00 AM EST (13:00 UTC). It is protected by a `CRON_SECRET` and triggers the `autoScraper.runAutoScraping()` function. It also schedules follow-up tasks via the `scheduler` module.
    - **CCR:** C:4, C:9, R:5
    - **Verification Plan:**
      - **1. Check Vercel Logs:** After 8:00 AM EST, inspect the Vercel deployment logs for the `/api/cron/auto-scrape` endpoint. Look for successful (200) or failed (500) status codes.
      - **2. Check Supabase `activity_logs`:** Query the `activity_logs` table for `auto_scrape_started` and `auto_scrape_completed` or `auto_scrape_failed` entries.
      - **3. Check `food_trucks` data:** Verify that the `last_scraped_at` timestamps for trucks have been recently updated.
  - **[ ] 3.1.3: Analyze `quality-check` Job**
    - **Guidance:** The CRON job at `/api/cron/quality-check` runs daily at 8:00 AM EST (13:00 UTC). It is also protected by the `CRON_SECRET`. It fetches all trucks, calculates quality scores using `DataQualityService`, and batch updates the scores in the database.
    - **CCR:** C:4, C:9, R:5
    - **Verification Plan:**
      - **1. Check Vercel Logs:** After 8:00 AM EST, inspect the Vercel deployment logs for the `/api/cron/quality-check` endpoint.
      - **2. Check Supabase `activity_logs`:** Query the `activity_logs` table for `quality_check_started` and `quality_check_completed` or `quality_check_failed` entries.
      - **3. Check `food_trucks` data:** Verify that the `quality_score` column for trucks has been recently updated.
  - **[ ] 3.1.4: Create Checkpoint for CRON Job Verification**
    - **Guidance:** Before making any changes to the CRON jobs or their underlying code, create a checkpoint. This could be a git commit, a database backup, or both. This will allow for a quick rollback if verification fails.
    - **CCR:** C:2, C:10, R:2
    - **Verification:** A checkpoint is created and noted in the project documentation.

- **[ ] 3.2: Refine Data Categorization**
  - **Guidance:** The pipeline is incorrectly identifying directories ("Food Trucks in Charleston SC") and events ("Black Food Truck Festival") as food trucks. We need to build a classification system.
  - **CCR:** C:8, C:6, R:7
  - **Verification:** The database correctly distinguishes between `trucks`, `events`, and `source_directories`. The main user-facing list only shows trucks.
  - **[x] 3.2.1: Research Data Classification Techniques**
    - **Guidance:** Investigate methods to programmatically classify a scraped entity. This could involve keyword analysis, URL structure analysis, or using an AI service like Gemini for classification.
    - **CCR:** C:7, C:6, R:6
    - **Verification:** A chosen strategy is documented in `docs/DATA_PIPELINE_ARCHITECTURE.md`.
  - **[ ] 3.2.2: Implement Classifier in Scraping Engine**
    - **Guidance:** Modify the scraping and processing logic (`lib/pipelineProcessor.ts`) to include the new classification step.
    - **CCR:** C:8, C:7, R:7
    - **Verification:** New scrapes correctly categorize entities.
    - **[ ] 3.2.2.1: Implement URL Pattern Analysis**
      - **Guidance:** Add logic to the `pipelineProcessor` to check for keywords in the URL.
      - **CCR:** C:3, C:9, R:3
      - **Verification:** The function correctly classifies URLs based on predefined patterns.
    - **[ ] 3.2.2.2: Implement On-Page Keyword Analysis**
      - **Guidance:** Add logic to extract and analyze keywords from `<h1>`, `<h2>`, and `<title>` tags.
      - **CCR:** C:4, C:8, R:4
      - **Verification:** The function correctly classifies pages based on on-page keywords.
    - **[ ] 3.2.2.3: Implement Gemini API Fallback**
      - **Guidance:** Integrate the Gemini API call as a fallback for when the heuristic methods are not confident.
      - **CCR:** C:5, C:8, R:6
      - **Verification:** The Gemini API is called when needed and returns the correct classification.
  - **[ ] 3.2.3: Create Schema for Events and Directories**
    - **Guidance:** Update the Supabase schema to include tables for `events` and `source_directories`.
    - **CCR:** C:5, C:9, R:4
    - **Verification:** The new tables exist in Supabase.
    - **[ ] 3.2.3.1: Draft SQL for New Tables**
      - **Guidance:** Write the `CREATE TABLE` statements for the `events` and `source_directories` tables.
      - **CCR:** C:3, C:9, R:3
      - **Verification:** The SQL is syntactically correct.
    - **[ ] 3.2.3.2: Apply Schema Changes via Migration**
      - **Guidance:** Create a new Supabase migration file with the SQL for the new tables and apply it.
      - **CCR:** C:3, C:9, R:4
      - **Verification:** The new tables are visible in the Supabase dashboard.

- **[ ] 3.3: Implement Geocoding for Addresses**
  - **Guidance:** Some trucks will only have a physical address. We need a process to convert these addresses into latitude and longitude to display them on the map.
  - **CCR:** C:7, C:8, R:6
  - **Verification:** Trucks with only an address are visible on the map in the correct location.
  - **[x] 3.3.1: Research Geocoding Services**
    - **Guidance:** Evaluate free and open-source geocoding services. Options could include Nominatim (OpenStreetMap), or a free tier of a commercial service. Check Supabase for built-in PostGIS functions that might help.
    - **CCR:** C:5, C:7, R:5
    - **Verification:** A service is chosen and documented in `docs/DATA_PIPELINE_ARCHITECTURE.md`.
  - **[ ] 3.3.2: Integrate Geocoding into Data Pipeline**
    - **Guidance:** Add a step in the pipeline that checks if GPS coordinates are missing and, if so, calls the chosen geocoding service to populate them.
    - **CCR:** C:7, C:8, R:6
    - **Verification:** The `latitude` and `longitude` columns in the `trucks` table are populated for address-only entries after a scrape.
    - **[ ] 3.3.2.1: Enable PostGIS Extensions in Supabase**
      - **Guidance:** Run the `CREATE EXTENSION` commands for `postgis` and `postgis_tiger_geocoder` in the Supabase SQL Editor.
      - **CCR:** C:2, C:9, R:3
      - **Verification:** The extensions are successfully enabled.
    - **[ ] 3.3.2.2: Create Geocoding Trigger Function**
      - **Guidance:** Write and apply the SQL for the trigger function that will automatically geocode addresses.
      - **CCR:** C:4, C:8, R:5
      - **Verification:** The function is created in Supabase.
    - **[ ] 3.3.2.3: Attach Trigger to `food_trucks` Table**
      - **Guidance:** Write and apply the `CREATE TRIGGER` SQL statement.
      - **CCR:** C:3, C:9, R:4
      - **Verification:** The trigger is attached to the table and fires on `INSERT` or `UPDATE`.
    - **[ ] 3.3.2.4: Implement Nominatim Fallback Logic**
      - **Guidance:** In `lib/pipelineProcessor.ts`, add the logic to call the Nominatim API if the database geocoding fails.
      - **CCR:** C:5, C:8, R:5
      - **Verification:** The fallback logic is implemented and correctly updates the database.

- **[ ] 3.4: Define and Refine Data Quality Score**
  - **Guidance:** The current quality score is ambiguous. We need to define the specific metrics, weights, and thresholds that contribute to the score.
  - **CCR:** C:6, C:5, R:5
  - **Verification:** The logic for calculating the quality score is documented and implemented, and the scores in the admin dashboard reflect the new, clear metrics.
  - **[x] 3.4.1: Research Best Practices for Data Quality Metrics**
    - **Guidance:** Look at how other data-driven applications quantify data quality. Identify key fields for a food truck (e.g., has menu, has recent location, has photo, has operating hours) and assign weights to them.
    - **CCR:** C:4, C:6, R:4
    - **Verification:** A formula and a list of weighted metrics are documented in `docs/DATA_QUALITY_GUIDE.md`.
  - **[ ] 3.4.2: Implement New Quality Score Logic**
    - **Guidance:** Update the backend script that calculates the quality scores based on the new formula.
    - **CCR:** C:6, C:8, R:5
    - **Verification:** The scores are recalculated and updated in the database.
    - **[ ] 3.4.2.1: Refactor `DataQualityService`**
      - **Guidance:** Update the `calculateQualityScore` method in `lib/utils/QualityScorer.ts` to use the new weighted system from `docs/DATA_QUALITY_GUIDE.md`.
      - **CCR:** C:4, C:8, R:4
      - **Verification:** The method correctly calculates scores based on the new logic.
    - **[ ] 3.4.2.2: Update `quality-check` CRON Job**
      - **Guidance:** Ensure the `quality-check` CRON job correctly calls the updated `DataQualityService` and stores the new scores.
      - **CCR:** C:3, C:9, R:4
      - **Verification:** The CRON job runs successfully and updates the `quality_score` column in the `food_trucks` table.

---

## 4.0 UI/UX Overhaul & Frontend Features

- **[ ] 4.1: Redesign Core UI Components**
  - **Guidance:** The current UI is described as "boxy" and "square". The goal is to create a more modern, rounded, and visually appealing design. This will likely involve updating TailwindCSS configuration and component styles.
  - **CCR:** C:7, C:7, R:5
  - **Verification:** The application's UI reflects the new design language.
  - **[x] 4.1.1: Research UI Frameworks and Design Systems**
    - **Guidance:** Investigate "Magic UI" and other open-source component libraries/frameworks that can help achieve the desired "black glassmorphism with red neon highlights" aesthetic.
    - **CCR:** C:5, C:6, R:4
    - **Verification:** A decision on whether to use a library or build custom styles is made and documented in `docs/UI_DESIGN_GUIDE.md`.
  - **[ ] 4.1.2: Update Base Styles and Component Library**
    - **Guidance:** Modify `app/globals.css`, `tailwind.config.ts`, and core UI components in `components/ui/` to use more `border-radius`, new color schemes, and other modern styling.
    - **CCR:** C:7, C:8, R:5
    - **Verification:** Components like Cards, Buttons, and Badges have a new, rounded look.
    - **[ ] 4.1.2.1: Install and Configure `glasscn-ui`**
      - **Guidance:** Add the `glasscn-ui` package and update `tailwind.config.ts` and `globals.css` as per the library's documentation.
      - **CCR:** C:3, C:9, R:4
      - **Verification:** The library is installed and configured without errors.
    - **[ ] 4.1.2.2: Refactor `Card` Component**
      - **Guidance:** Update the `Card` component in `components/ui/` to use the `glass` variant from `glasscn-ui`.
      - **CCR:** C:3, C:9, R:3
      - **Verification:** The `Card` component now has a glassmorphism effect.
    - **[ ] 4.1.2.3: Refactor `Button` Component**
      - **Guidance:** Update the `Button` component to incorporate neon glow effects on hover and focus, using custom styles in `globals.css`.
      - **CCR:** C:4, C:8, R:4
      - **Verification:** The `Button` component has the desired neon effect.

- **[ ] 4.2: Improve "View Details" Experience**
  - **Guidance:** Clicking "View Details" should not navigate to a new page. It should display the information on the same page, using a modal, accordion, or similar non-disruptive UI pattern.
  - **CCR:** C:6, C:9, R:4
  - **Verification:** The user can view truck details without a full page load.
  - **[ ] 4.2.1: Implement Non-Navigational Detail View**
    - **Guidance:** Refactor the `TruckCard` and `TruckListSection` components to open details in a modal (`AlertDialog` or `Sheet` from `components/ui`) or by expanding the card itself.
    - **CCR:** C:6, C:9, R:4
    - **Verification:** The new detail view is functional and smooth.
  - **[ ] 4.2.2: Hide Developer-Facing Data**
    - **Guidance:** Remove fields like "Coordinates" from the user-facing detail view. This data is useful for admins but not for regular users.
    - **CCR:** C:2, C:10, R:1
    - **Verification:** The detail view only shows user-friendly information.

- **[ ] 4.3: Replace Food Truck Icon**
  - **Guidance:** The current map marker for food trucks needs to be replaced with a better icon that clearly represents a food truck.
  - **CCR:** C:2, C:10, R:1
  - **Verification:** The map displays the new, improved food truck icon.
  - **[ ] 4.3.1: Source a New Food Truck Icon**
    - **Guidance:** Find a suitable, open-source (or free-to-use) SVG icon for a food truck.
    - **CCR:** C:1, C:10, R:0
    - **Verification:** A new SVG file is added to the `public/` directory.
  - **[ ] 4.3.2: Update Map Component to Use New Icon**
    - **Guidance:** Modify `components/map/MapComponent.tsx` to use the new icon for markers.
    - **CCR:** C:3, C:9, R:2
    - **Verification:** The new icon appears correctly on the map.

- **[ ] 4.4: Design and Implement User Portal**
  - **Guidance:** Create a user-specific area, likely accessed via a hamburger menu or user icon in the header. This will be the foundation for personalized features.
  - **CCR:** C:8, C:6, R:7
  - **Verification:** A basic user portal is accessible after login.
  - **[ ] 4.4.1: Plan User Portal Features (Phase 1)**
    - **Guidance:** For the initial version, plan for a minimal set of features: viewing favorited trucks and a settings page. Document plans for future features like notifications and coupons.
    - **CCR:** C:4, C:7, R:3
    - **Verification:** A feature list for the user portal is documented in `docs/USER_PORTAL_PLAN.md`.
  - **[ ] 4.4.2: Implement User Favorites**
    - **Guidance:** Add a "favorite" button to `TruckCard`. Create a new table in Supabase to store user favorites (`user_id`, `truck_id`). Display favorited trucks in the user portal.
    - **CCR:** C:7, C:8, R:6
    - **Verification:** Users can favorite and unfavorite trucks, and the list appears in their portal.
    - **[ ] 4.4.2.1: Create `user_favorites` Table**
      - **Guidance:** Write and apply a Supabase migration to create the `user_favorites` table with `user_id` and `truck_id` columns.
      - **CCR:** C:3, C:9, R:4
      - **Verification:** The table is created in Supabase.
    - **[ ] 4.4.2.2: Implement "Favorite" Button UI**
      - **Guidance:** Add a "favorite" button to the `TruckCard` component, with logic to handle the visual state (favorited/not favorited).
      - **CCR:** C:4, C:9, R:3
      - **Verification:** The button appears and changes state on click.
    - **[ ] 4.4.2.3: Implement Favorite/Unfavorite Logic**
      - **Guidance:** Create the client-side and server-side logic to add/remove entries from the `user_favorites` table when the button is clicked.
      - **CCR:** C:5, C:8, R:5
      - **Verification:** Clicking the button correctly updates the database.
    - **[ ] 4.4.2.4: Display Favorites in User Portal**
      - **Guidance:** Create a new component to fetch and display the user's favorited trucks in their portal.
      - **CCR:** C:4, C:9, R:4
      - **Verification:** The user's favorites are correctly displayed.

---

## 5.0 Admin Dashboard Enhancements

- **[ ] 5.1: Fix Data Discrepancies**
  - **Guidance:** The admin dashboard panels are not all showing the same data (e.g., 9 trucks vs. 10 trucks). This suggests a data fetching or state management issue.
  - **CCR:** C:7, C:6, R:7
  - **Verification:** All panels on the admin dashboard show consistent data that matches the database.
  - **[ ] 5.1.1: Investigate Data Fetching Logic**
    - **Guidance:** Review the data fetching code for each panel on the admin dashboard (`app/admin/page.tsx` and its components). Check if some panels are using cached data or different query filters.
    - **CCR:** C:6, C:7, R:6
    - **Verification:** The root cause of the discrepancy is identified and documented.
    - **[ ] 5.1.1.1: Analyze `TrucksPage` Component**
      - **Guidance:** Examine the data fetching in `components/admin/dashboard/TrucksPage.tsx`.
      - **CCR:** C:4, C:8, R:4
      - **Verification:** The data fetching logic is understood and documented.
    - **[ ] 5.1.1.2: Analyze Data Quality Components**
      - **Guidance:** Examine the data fetching in `app/admin/data-quality/page.tsx` and its child components.
      - **CCR:** C:4, C:8, R:4
      - **Verification:** The data fetching logic is understood and documented.
  - **[ ] 5.1.2: Unify Data Source or Refetching Strategy**
    - **Guidance:** Refactor the dashboard to use a single, consistent data source (e.g., a React context or a unified hook) or ensure all panels refetch data on a consistent trigger.
    - **CCR:** C:7, C:8, R:7
    - **Verification:** The fix is implemented and all panels update in sync.
    - **[ ] 5.1.2.1: Implement TanStack Query**
      - **Guidance:** Integrate TanStack Query (React Query) for server state management to ensure data is fetched and cached consistently across all components.
      - **CCR:** C:5, C:8, R:6
      - **Verification:** TanStack Query is set up and used for data fetching in the admin dashboard.
    - **[ ] 5.1.2.2: Refactor Components to Use `useQuery`**
      - **Guidance:** Replace the existing `useEffect`-based data fetching with the `useQuery` hook from TanStack Query.
      - **CCR:** C:4, C:9, R:4
      - **Verification:** All admin dashboard components use `useQuery` for data fetching.

- **[ ] 5.2: Improve Auto-Scraping Page**
  - **Guidance:** The auto-scraping page is vague. It needs to provide more useful information about scraping status and history.
  - **CCR:** C:6, C:7, R:5
  - **Verification:** The auto-scraping page is more informative and functional.
  - **[ ] 5.2.1: Display Scraping History**
    - **Guidance:** Create a table to log the status of each scraping batch job (start time, end time, status, number of trucks processed). Display this history on the auto-scraping page.
    - **CCR:** C:6, C:8, R:5
    - **Verification:** The page shows a history of past scraping jobs.
  - **[ ] 5.2.2: Implement "Trigger Manual Scrape" Functionality**
    - **Guidance:** The button currently does nothing. Hook it up to an API endpoint that triggers the scraping pipeline on demand. Provide feedback to the user that the scrape has started.
    - **CCR:** C:5, C:9, R:4
    - **Verification:** Clicking the button successfully initiates a new scraping job.

- **[ ] 5.3: Improve Data Quality Page UI**
  - **Guidance:** The charts on the Data Quality page are boxy and the pie chart labels overlap.
  - **CCR:** C:5, C:8, R:3
  - **Verification:** The charts are visually appealing and the labels are readable.
  - **[ ] 5.3.1: Fix Pie Chart Labels**
    - **Guidance:** Adjust the configuration of the charting library (likely Recharts) to position the labels correctly, perhaps below the chart with a legend.
    - **CCR:** C:4, C:9, R:2
    - **Verification:** The pie chart is legible.
  - **[ ] 5.3.2: Apply New Design to Charts**
    - **Guidance:** Update the chart components to match the new, rounded design language.
    - **CCR:** C:5, C:8, R:3
    - **Verification:** The charts have rounded corners and fit the new aesthetic.

---

## 6.0 Future-Proofing & Deployment

- **[ ] 6.1: Plan for Mobile App Conversion**
  - **Guidance:** Research methods for wrapping the Next.js web app into a mobile app for the Google Play Store, prioritizing cost-effective solutions.
  - **CCR:** C:7, C:5, R:6
  - **Verification:** A document, `docs/MOBILE_DEPLOYMENT_PLAN.md`, is created, comparing technologies like Progressive Web Apps (PWA), Capacitor, and others.
    - **[ ] 6.1.1: Research PWA Conversion**
      - **Guidance:** Investigate the steps required to make the Next.js app a fully-featured Progressive Web App.
      - **CCR:** C:4, C:7, R:4
      - **Verification:** The PWA conversion process is documented.
    - **[ ] 6.1.2: Research Capacitor Conversion**
      - **Guidance:** Investigate the steps required to wrap the app with Capacitor.
      - **CCR:** C:4, C:7, R:5
      - **Verification:** The Capacitor conversion process is documented.
- **[ ] 6.2: Plan for Payment Integration**
  - **Guidance:** Research and plan for the integration of a payment provider for food truck owner subscriptions. The preference is for Polar over Stripe.
  - **CCR:** C:6, C:6, R:8
  - **Verification:** A document, `docs/PAYMENT_INTEGRATION_PLAN.md`, is created, outlining the chosen provider, the required data schema changes, and the implementation steps.
    - **[ ] 6.2.1: Research Polar Payments**
      - **Guidance:** Investigate the Polar Payments API, pricing, and integration with Next.js.
      - **CCR:** C:4, C:7, R:5
      - **Verification:** The research is documented in `docs/PAYMENT_INTEGRATION_PLAN.md`.
    - **[ ] 6.2.2: Design Subscription Schema**
      - **Guidance:** Design the necessary Supabase tables to manage user subscriptions, plans, and payment statuses.
      - **CCR:** C:4, C:8, R:5
      - **Verification:** The schema is documented in `docs/PAYMENT_INTEGRATION_PLAN.md`.

---

## 7.0 Supabase Health & Optimization

- **[x] 7.1: Optimize RLS Policies**
  - **Guidance:** The database has multiple, overlapping permissive RLS policies for the same roles and actions, which hurts performance. These need to be consolidated. This is still critical even with an external auth provider.
  - **CCR:** C:7, C:6, R:7
  - **Verification:** The number of RLS policies is reduced, and queries on the affected tables show improved performance.
  - **[x] 7.1.1: Consolidate Multiple Permissive Policies**
    - **Guidance:** For tables like `events`, `food_truck_schedules`, and `menu_items`, combine the multiple `PERMISSIVE` policies for each action (`SELECT`, `INSERT`, `UPDATE`, `DELETE`) into a single, more efficient policy. This will involve creating more comprehensive `USING` and `WITH CHECK` expressions based on the user's role derived from their JWT.
    - **CCR:** C:7, C:6, R:7
    - **Verification:** The redundant policies are removed from the Supabase dashboard, leaving one policy per action/role.

- **[x] 7.2: Optimize Database Indexing**
  - **Guidance:** Address the indexing warnings from the Supabase Advisor to improve query performance and remove dead weight.
  - **CCR:** C:6, C:7, R:6
  - **Verification:** The indexing-related warnings are no longer present in the Supabase Advisor report.
  - **[x] 7.2.1: Add Indexes to Foreign Keys**
    - **Guidance:** Add indexes to the foreign key columns identified in the advisor report (`data_processing_queue.truck_id`, `events.food_truck_id`, `food_truck_schedules.food_truck_id`). This can be done via the Supabase SQL editor.
    - **CCR:** C:4, C:9, R:4
    - **Verification:** The new indexes are visible in the Supabase table definitions.
  - **[ ] 7.2.2: Remove Unused Indexes**
    - **Guidance:** The advisor has identified a large number of unused indexes across various tables (`api_usage`, `discovered_directories`, `discovered_urls`, `food_trucks`, `scraping_jobs`). These should be reviewed and dropped to save space and reduce maintenance overhead.
    - **CCR:** C:5, C:8, R:5
    - **Verification:** The unused indexes are removed, and application functionality remains unaffected.
    - **[ ] 7.2.2.1: Analyze Index Usage**
      - **Guidance:** Use the `pg_stat_user_indexes` view in Supabase to confirm that the identified indexes have zero or very low usage.
      - **CCR:** C:3, C:8, R:3
      - **Verification:** The analysis confirms the advisor's findings.
    - **[ ] 7.2.2.2: Draft `DROP INDEX` Statements**
      - **Guidance:** Write the `DROP INDEX` statements for the confirmed unused indexes.
      - **CCR:** C:2, C:9, R:4
      - **Verification:** The SQL is syntactically correct.
    - **[ ] 7.2.2.3: Apply Index Deletions via Migration**
      - **Guidance:** Create a new Supabase migration file with the `DROP INDEX` statements and apply it.
      - **CCR:** C:3, C:9, R:5
      - **Verification:** The indexes are removed from the database.
