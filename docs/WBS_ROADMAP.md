# Food Truck Finder - WBS & Strategic Roadmap (REFERENCE)

**⚠️ STATUS: REFERENCE DOCUMENT**  
**Active Planning Document:** [`docs/PROJECT_PLAN.md`](PROJECT_PLAN.md)

*This document contains detailed task breakdowns for reference but is no longer the active planning document.*

## Project Overview & Guiding Principles

**Vision:** To engineer a premier, secure, and user-centric platform for discovering food trucks. Our goal is to create an application that is not only functionally robust but also aesthetically pleasing and trustworthy, providing immediate value to both consumers and food truck owners.

**Current State:** The application is a functional MVP with a live data pipeline, a basic UI, and an admin dashboard. However, it requires significant refinement across several key areas. The UI is "boxy" and lacks a modern aesthetic; the data pipeline has known accuracy issues in entity classification and location precision; the admin dashboard has data synchronization problems and is currently unsecured.

**Core Pillars of Development:** All tasks in this roadmap are aligned with four core pillars:
1.  **Security & Access Control:** Implementing a robust, single-user admin security model and proactively identifying vulnerabilities.
2.  **Data Integrity & Pipeline Reliability:** Ensuring the data is accurate, up-to-date, and correctly classified. This includes verifying our CRON automation and improving geocoding.
3.  **UI/UX Modernization:** Transforming the user interface into a visually appealing, intuitive, and seamless experience, guided by a "black glassmorphism with red neon highlights" aesthetic.
4.  **Strategic Future-Proofing:** Planning for future growth, including a user portal, payment systems, and mobile app deployment, while prioritizing cost-effective, open-source solutions.

**Methodology:** All development will strictly adhere to the **Zero-Trust Planning & Execution Protocol**. This means every task is meticulously researched, broken down into atomic units with clear verification steps, and subjected to a post-action verification cycle (`tsc`, `eslint`, `jscpd`) to maintain codebase health.

**Git Workflow:** All major feature development (corresponding to a main section of this WBS, e.g., 2.0, 3.0) will be done in a dedicated feature branch. This ensures `main` remains stable and all changes are isolated until fully verified. Atomic commits will be used for each sub-task to maintain a clean and traceable history.

---

## 1.0 Project Management & Foundation

- **[ ] 1.1: Establish and Maintain WBS Roadmap**
  - **Guidance:** This task involves the continuous refinement of this WBS document. As we research and complete tasks, we will update the status, add new sub-tasks, and adjust priorities. This document is our single source of truth for the project plan.
  - **CCR:** C:1, C:10, R:1
  - **Verification:** The WBS is regularly updated after each major task completion.

- **[x] 1.2: Review Existing Project Documentation**
  - **Guidance:** Thoroughly read and understand the contents of `supabase_advisor.md` and `supabase_improvement_plan.md` to identify immediate, actionable items recommended by Supabase.
  - **CCR:** C:2, C:10, R:1
  - **Verification:** A summary of key findings from these documents is created and integrated into the relevant sections of this WBS.

- **[ ] 1.3: Enforce Zero-Trust Verification Protocol**
  - **Guidance:** For all subsequent tasks, strictly adhere to the "Zero-Trust Post-Action Verification Protocol." After every 10-15 significant edits, run the holy trinity of checks: `npx tsc --noEmit`, `npx eslint .`, and `npx jscpd .` to ensure we are not introducing new errors.
  - **CCR:** C:2, C:10, R:2
  - **Verification:** Command history and task outputs demonstrate regular execution of these verification checks.

---

## 2.0 Security & Access Control (REVISED - PHASE 1 SIMPLIFIED)

  - **[ ] 2.1: Secure Admin Dashboard (SIMPLIFIED FOR BETA)**
    - **Guidance:** REVISED APPROACH - Instead of complex Firebase+Supabase auth, implement simple password protection for closed beta launch. This gets us secure admin access in 1-2 days instead of 2+ weeks.
    - **CCR:** C:3, C:9, R:4 -> **Action:** Simplified approach for immediate launch readiness.
    - **Verification:** Admin routes protected by environment variable password, unauthorized access blocked.
    - **Future:** Complex RBAC system moved to Phase 3 (public launch).
  - **[x] 2.1.1: Research Authentication Provider**
    - **Guidance:** Per user direction, the primary authentication provider will be Google, managed through a service like Firebase Authentication to decouple it from Supabase. Research the integration of Firebase Auth with a Supabase backend. The goal is to use Firebase for sign-in and then pass the user's identity (JWT) to Supabase to control data access via RLS.
    - **Tools/Libraries:** Firebase Authentication, Firebase Cloud Functions, Firebase Admin SDK, Supabase Client Library.
    - **CCR:** C:3, C:10, R:2
    - **Verification:** A new document, `docs/AUTH_ARCHITECTURE.md`, is created detailing the chosen implementation strategy.
  - **[x] 2.1.2: Research Role-Based Access Control (RBAC) with External JWT**
    - **Guidance:** Investigate best practices for implementing RBAC in Supabase using a JWT from an external provider (like Firebase). This involves setting up a custom `auth.users` table or a separate `profiles` table with a `role` column, and creating RLS policies that check this role.
    - **CCR:** C:3, C:10, R:2
    - **Verification:** A clear plan for managing admin roles is added to `docs/AUTH_ARCHITECTURE.md`.
  - **[ ] 2.1.3: Implement Supabase RBAC Schema**
    - **Guidance:** Create the necessary SQL types and tables in Supabase to support a permission-based RBAC system, as detailed in `docs/AUTH_ARCHITECTURE.md`.
    - **CCR:** C:4, C:9, R:4 -> **Action:** Break down.
    - **Impact Analysis:**
      - **Associated Files:** `supabase/migrations/<timestamp>_create_rbac_schema.sql`
      - **Affected Files:** All RLS policies, `app/middleware.ts`, any future components that manage roles.
    - **Verification:** The `app_role` and `app_permission` types, and the `role_permissions` table are successfully created in the Supabase database.
    - **[ ] 2.1.3.1: Draft `app_role` and `app_permission` ENUM Types**
      - **Guidance:** Write the `CREATE TYPE` SQL statements for `app_role` (`admin`, `authenticated`) and `app_permission` (`trucks.read`, `trucks.write`, etc.).
      - **CCR:** C:1, C:10, R:1
      - **Verification:** The SQL is written and syntactically correct.
    - **[ ] 2.1.3.2: Draft `role_permissions` Table**
      - **Guidance:** Write the `CREATE TABLE` SQL for the `role_permissions` table, linking roles to permissions.
      - **CCR:** C:1, C:10, R:1
      - **Verification:** The SQL is written and syntactically correct.
    - **[ ] 2.1.3.3: Create and Apply RBAC Schema Migration**
      - **Guidance:** Combine the SQL from the previous two steps into a single migration file and apply it using the Supabase CLI.
      - **CCR:** C:2, C:10, R:3
      - **Verification:** The new types and table appear in the Supabase dashboard.
  - **[ ] 2.1.4: Implement Supabase `authorize` Function**
    - **Guidance:** Create the `public.authorize` SQL function in Supabase. This function will be the central point for checking user permissions in all RLS policies.
    - **CCR:** C:5, C:8, R:6 -> **Action:** Break down.
    - **Impact Analysis:**
      - **Associated Files:** `supabase/migrations/<timestamp>_create_authorize_function.sql`
      - **Affected Files:** All RLS policies, any tests that simulate user roles.
    - **Verification:** The `authorize` function is created and returns the expected boolean values when tested with different roles and permissions in the Supabase SQL Editor.
    - **[ ] 2.1.4.1: Draft `authorize` Function SQL**
      - **Guidance:** Write the initial SQL for the function based on the architecture defined in `docs/AUTH_ARCHITECTURE.md`. It should extract the `role` from the JWT and check against the `role_permissions` table.
      - **CCR:** C:3, C:9, R:3
      - **Verification:** The SQL code is written and passes a syntax check.
    - **[ ] 2.1.4.2: Test `authorize` Function with Mock JWTs**
      - **Guidance:** In the Supabase SQL Editor, use the `set_config` command to simulate JWTs with different roles (`admin`, `authenticated`) and test the function's output against various permissions.
      - **CCR:** C:3, C:10, R:3
      - **Verification:** The function returns `true` for authorized roles and `false` for unauthorized roles.
    - **[ ] 2.1.4.3: Security Review of `authorize` Function SQL**
      - **Guidance:** Before migration, perform a dedicated security review of the final SQL code for the `authorize` function. Check for any potential logic flaws, edge cases, or possibilities for privilege escalation.
      - **CCR:** C:3, C:9, R:4
      - **Verification:** The review is completed and signed off on in the project documentation.
    - **[ ] 2.1.4.4: Create and Apply `authorize` Function Migration**
      - **Guidance:** Place the reviewed SQL into a new migration file and apply it.
      - **CCR:** C:2, C:10, R:3
      - **Verification:** The function is visible and usable in the Supabase SQL Editor.
  - **[ ] 2.1.5: Refactor RLS Policies to Use `authorize` Function**
    - **Guidance:** Update all existing RLS policies on tables like `trucks`, `events`, etc., to use the new `authorize` function instead of direct role checks.
    - **CCR:** C:6, C:7, R:7 -> **Action:** Break down.
    - **Impact Analysis:**
      - **Associated Files:** `supabase/migrations/<timestamp>_refactor_rls_policies.sql`
      - **Affected Files:** All code performing CRUD operations on tables with RLS (`trucks`, `events`, `menu_items`, etc.), all integration and e2e tests.
    - **Verification:** RLS policies are updated, and data access rules are correctly enforced for different user roles.
    - **[ ] 2.1.5.1: Refactor `trucks` Table RLS**
      - **Guidance:** Update the RLS policies for the `trucks` table to use `authorize(auth.uid(), 'trucks.read')` for `SELECT`, `authorize(auth.uid(), 'trucks.write')` for `INSERT`/`UPDATE`, etc.
      - **CCR:** C:2, C:10, R:3
      - **Verification:** The policies are updated and tested in the SQL Editor.
    - **[ ] 2.1.5.2: Refactor `events` Table RLS**
      - **Guidance:** Update the RLS policies for the `events` table similarly.
      - **CCR:** C:2, C:10, R:3
      - **Verification:** The policies are updated and tested.
    - **[ ] 2.1.5.3: Refactor `menu_items` Table RLS**
      - **Guidance:** Update the RLS policies for the `menu_items` table similarly.
      - **CCR:** C:2, C:10, R:3
      - **Verification:** The policies are updated and tested.
    - **[ ] 2.1.5.4: Create and Apply RLS Refactor Migration**
      - **Guidance:** Consolidate all RLS policy changes into a single migration file and apply it.
      - **CCR:** C:2, C:10, R:4
      - **Verification:** The new policies are active in the Supabase dashboard.
  - **[ ] 2.1.6: Configure Firebase and Supabase Integration**
    - **Guidance:** Follow the steps in `docs/AUTH_ARCHITECTURE.md` to add the Firebase Project ID as a trusted JWT issuer in the Supabase dashboard.
    - **CCR:** C:2, C:10, R:3
    - **Verification:** The integration is successfully created in the Supabase dashboard.
  - **[ ] 2.1.7: Implement Firebase Cloud Function for Custom Claims**
    - **Guidance:** Deploy a `beforeUserCreated` blocking function in Firebase to assign a default `authenticated` role to new users.
    - **CCR:** C:6, C:7, R:6 -> **Action:** Break down.
    - **Verification:** New users created via Firebase Auth have the `role: 'authenticated'` custom claim in their JWT.
    - **[ ] 2.1.7.1: Write Cloud Function Code**
      - **Guidance:** Write the Node.js code for the `beforeUserCreated` function. It should take the user record and add `customClaims: { role: 'authenticated' }`.
      - **CCR:** C:3, C:10, R:3
      - **Verification:** The code is written and passes linting.
    - **[ ] 2.1.7.2: Deploy and Test Cloud Function**
      - **Guidance:** Deploy the function to Firebase and test the user creation flow.
      - **CCR:** C:3, C:9, R:4
      - **Verification:** A new test user is created, and their JWT is inspected to confirm the custom claim.
  - **[ ] 2.1.8: Manually Assign Admin Role in Firebase**
    - **Guidance:** Using the Firebase Admin SDK in a secure, one-off script, assign the `admin` role to the designated admin user's Firebase UID.
    - **CCR:** C:3, C:9, R:5
    - **Verification:** The admin user's JWT contains the `role: 'admin'` custom claim.
  - **[ ] 2.1.9: Configure Next.js Supabase Client for Firebase JWT**
    - **Guidance:** Update the Supabase client initialization in the Next.js app to dynamically use the JWT from the authenticated Firebase user.
    - **CCR:** C:5, C:8, R:6 -> **Action:** Break down.
    - **Verification:** The Supabase client successfully authenticates requests using the Firebase JWT.
    - **[ ] 2.1.9.1: Create a Firebase Auth Context**
      - **Guidance:** Implement a React context to manage the Firebase user state and provide the JWT to the application.
      - **CCR:** C:3, C:10, R:3
      - **Verification:** The context is created and provides the user's JWT.
    - **[ ] 2.1.9.2: Update Supabase Client to Use Context**
      - **Guidance:** Modify the Supabase client to retrieve the JWT from the Firebase Auth Context's `getAccessToken` method.
      - **CCR:** C:3, C:9, R:4
      - **Verification:** The Supabase client is successfully initialized with the JWT.
  - **[ ] 2.1.10: Implement Middleware for Route Protection**
    - **Guidance:** Create or modify `app/middleware.ts` to check for a user's session (via Firebase) and their `admin` role (from the JWT) before allowing access to `/admin/*` paths.
    - **CCR:** C:6, C:8, R:6 -> **Action:** Break down.
    - **Impact Analysis:**
      - **Associated Files:** `app/middleware.ts`, `lib/auth/utils.ts` (potentially new)
      - **Affected Files:** All routes under `/admin`, `app/login/page.tsx`, Supabase client configuration, E2E tests for admin section.
    - **Verification:** Middleware correctly intercepts and redirects non-admin users away from admin routes.
    - **[ ] 2.1.10.1: Draft Middleware Logic**
      - **Guidance:** Write the core logic for the middleware to get the session cookie, verify it with Firebase Admin SDK, decode the JWT, and check for the `admin` role.
      - **CCR:** C:3, C:9, R:4
      - **Verification:** The logic is written and passes linting.
    - **[ ] 2.1.10.2: Implement Redirect for Unauthorized Users**
      - **Guidance:** Add the redirect logic to the middleware to send unauthorized users to the `/access-denied` page.
      - **CCR:** C:2, C:10, R:2
      - **Verification:** Unauthorized users are correctly redirected.

- **[ ] 2.2: Research and Plan for AI Red Teaming**
  - **Guidance:** Locate the GitHub repository for "AI Red Teaming with MCP" and research how to set it up. The goal is to proactively identify security vulnerabilities in the codebase, especially before implementing payment features.
  - **Tools/Libraries:** GH05TCREW/PentestMCP, MCP-Mirror/enkryptai_enkryptai-mcp-server
  - **CCR:** C:6, C:8, R:7 -> **Action:** Break down.
  - **Verification:** A new document, `docs/SECURITY_PLAN.md`, is created, outlining the plan for red teaming, including setup instructions and a schedule for running scans.
    - **[ ] 2.2.1: Locate and Analyze Red Teaming Repository**
      - **Guidance:** Find the specified GitHub repository and analyze its README and documentation to understand its capabilities and setup process.
      - **CCR:** C:2, C:9, R:2
      - **Verification:** The repository is located and a summary of its features is added to `docs/SECURITY_PLAN.md`.
    - **[ ] 2.2.2: Document Setup and Configuration**
      - **Guidance:** Detail the step-by-step process for installing, configuring, and running the AI Red Teaming tool against our codebase.
      - **CCR:** C:3, C:8, R:4
      - **Verification:** The setup instructions are clearly documented in `docs/SECURITY_PLAN.md`.
    - **[ ] 2.2.3: Define Red Teaming Schedule and Scope**
      - **Guidance:** Outline a schedule for running the red teaming scans (e.g., before major releases, on a quarterly basis) and define the initial scope of the scans.
      - **CCR:** C:2, C:9, R:3
      - **Verification:** The schedule and scope are documented in `docs/SECURITY_PLAN.md`.

- **[ ] 2.3: Verification Checkpoint: Security**
  - **Guidance:** After completing all tasks in section 2.0, perform a full verification cycle.
  - **CCR:** C:2, C:10, R:4
  - **Verification:**
    - **[ ] 2.3.1: Run `npx tsc --noEmit`** - Confirm no new TypeScript errors.
    - **[ ] 2.3.2: Run `npx eslint .`** - Confirm no new linting errors.
    - **[ ] 2.3.3: Run `npx jscpd .`** - Confirm no new code duplication.
    - **[ ] 2.3.4: Manual E2E Test** - Manually test the login flow for both admin and non-admin users. Confirm admin access is granted and non-admin access is denied.

---

## 3.0 Core Data Pipeline & Quality

- **[ ] 3.1: Verify Vercel CRON Job**
  - **Guidance:** The automated scraping process is the lifeblood of this app. We need to confirm the Vercel CRON job that triggers the scraping is running correctly and on schedule.
  - **Tools/Libraries:** Vercel Deployment Logs, `curl`.
  - **CCR:** C:6, C:7, R:8 -> **Action:** Break down.
  - **Common Pitfalls & Fallbacks:**
    - **Caching Issues:** The CRON job endpoint might return a cached response, preventing the job from running. **Fallback:** Add `export const dynamic = 'force-dynamic';` to the route handler to prevent caching.
    - **Redirects:** CRON jobs do not follow redirects. **Fallback:** Ensure the CRON job path in `vercel.json` is the direct, final URL of the API route.
    - **Production Only:** CRON jobs only run on production deployments. **Fallback:** Use `curl` with the `CRON_SECRET` to test the endpoint in development and preview environments.
  - **Impact Analysis:**
    - **Associated Files:** `vercel.json`, `app/api/cron/auto-scrape/route.ts`, `app/api/cron/quality-check/route.ts`, `lib/autoScraper.ts`, `lib/utils/QualityScorer.ts`, `.env.local`
    - **Affected Files:** Supabase `food_trucks` and `activity_logs` tables, `lib/scheduler.ts`, Vercel deployment configuration.
  - **Verification:** Vercel logs show the CRON job executing successfully at the expected intervals. The "last scraped at" timestamps in the database are updated as expected.
    - **[ ] 3.1.1: Research CRON Job Configuration**
      - **Guidance:** Review `vercel.json` to identify all configured CRON jobs, their paths, and their schedules.
      - **CCR:** C:1, C:10, R:0
      - **Verification:** All CRON jobs are identified and listed in the sub-tasks below.
    - **[ ] 3.1.2: Analyze `auto-scrape` Job**
      - **Guidance:** The CRON job at `/api/cron/auto-scrape` runs daily at 8:00 AM EST (13:00 UTC). It is protected by a `CRON_SECRET` and triggers the `autoScraper.runAutoScraping()` function. It also schedules follow-up tasks via the `scheduler` module.
      - **CCR:** C:3, C:9, R:4
      - **Verification Plan:**
        - **1. Check Vercel Logs:** After 8:00 AM EST, inspect the Vercel deployment logs for the `/api/cron/auto-scrape` endpoint. Look for successful (200) or failed (500) status codes.
        - **2. Check Supabase `activity_logs`:** Query the `activity_logs` table for `auto_scrape_started` and `auto_scrape_completed` or `auto_scrape_failed` entries.
        - **3. Check `food_trucks` data:** Verify that the `last_scraped_at` timestamps for trucks have been recently updated.
    - **[ ] 3.1.3: Analyze `quality-check` Job**
      - **Guidance:** The CRON job at `/api/cron/quality-check` runs daily at 8:00 AM EST (13:00 UTC). It is also protected by the `CRON_SECRET`. It fetches all trucks, calculates quality scores using `DataQualityService`, and batch updates the scores in the database.
      - **CCR:** C:3, C:9, R:4
      - **Verification Plan:**
        - **1. Check Vercel Logs:** After 8:00 AM EST, inspect the Vercel deployment logs for the `/api/cron/quality-check` endpoint.
        - **2. Check Supabase `activity_logs`:** Query the `activity_logs` table for `quality_check_started` and `quality_check_completed` or `quality_check_failed` entries.
        - **3. Check `food_trucks` data:** Verify that the `quality_score` column for trucks has been recently updated.
    - **[ ] 3.1.4: Create Checkpoint for CRON Job Verification**
      - **Guidance:** Before making any changes to the CRON jobs or their underlying code, create a checkpoint. This could be a git commit, a database backup, or both. This will allow for a quick rollback if verification fails.
      - **CCR:** C:1, C:10, R:2
      - **Verification:** A checkpoint is created and noted in the project documentation.
    - **[ ] 3.1.5: Manually Trigger CRON Jobs for Testing**
      - **Guidance:** Instead of waiting for the scheduled time, use `curl` or a similar tool to send a POST request with the `CRON_SECRET` to the `/api/cron/auto-scrape` and `/api/cron/quality-check` endpoints to test them on demand.
      - **CCR:** C:2, C:10, R:3
      - **Verification:** The manual requests trigger the jobs, and the expected logs and database changes occur.
    - **[ ] 3.1.6: Monitor Production CRON Job Execution**
      - **Guidance:** After deploying the changes, monitor the Vercel logs for the first few scheduled runs at 8:00 AM EST to confirm they are executing automatically.
      - **CCR:** C:2, C:9, R:3
      - **Verification:** The jobs run successfully without manual intervention.

- **[ ] 3.2: Refine Data Categorization**
  - **Guidance:** The pipeline is incorrectly identifying directories ("Food Trucks in Charleston SC") and events ("Black Food Truck Festival") as food trucks. We need to build a classification system.
  - **CCR:** C:8, C:6, R:7 -> **Action:** Break down.
  - **Common Pitfalls & Fallbacks:**
    - **Incorrect Classification:** The Gemini API may misclassify content. **Fallback:** Implement a confidence score threshold. If the score is too low, flag the item for manual review in the admin dashboard.
    - **API Errors:** The Gemini API call might fail due to network issues or rate limiting. **Fallback:** Implement an exponential backoff retry mechanism. If it still fails, fall back to the less accurate heuristic methods (keyword/URL analysis).
    - **Data Privacy:** Sending scraped data to a third-party API has privacy implications. **Fallback:** Ensure no personally identifiable information (PII) is sent to the Gemini API. Anonymize data where possible.
  - **Impact Analysis:**
    - **Associated Files:** `lib/pipelineProcessor.ts`, `lib/gemini.ts`, `supabase/migrations/<timestamp>_create_events_tables.sql`, `docs/DATA_PIPELINE_ARCHITECTURE.md`
    - **Affected Files:** `food_trucks` table, new `events` and `source_directories` tables, frontend components displaying truck lists, admin dashboard.
  - **Verification:** The database correctly distinguishes between `trucks`, `events`, and `source_directories`. The main user-facing list only shows trucks.
  - **[x] 3.2.1: Research Data Classification Techniques**
    - **Guidance:** Investigate methods to programmatically classify a scraped entity. This could involve keyword analysis, URL structure analysis, or using an AI service like Gemini for classification.
    - **Tools/Libraries:** Google Gemini API, Heuristic text analysis.
    - **CCR:** C:3, C:9, R:3
    - **Verification:** A chosen strategy is documented in `docs/DATA_PIPELINE_ARCHITECTURE.md`.
  - **[ ] 3.2.2: Implement Classifier in Scraping Engine**
    - **Guidance:** Modify the scraping and processing logic (`lib/pipelineProcessor.ts`) to include the new classification step.
    - **CCR:** C:7, C:8, R:7 -> **Action:** Break down.
    - **Verification:** New scrapes correctly categorize entities.
    - **[ ] 3.2.2.1: Implement URL Pattern Analysis**
      - **Guidance:** Add logic to the `pipelineProcessor` to check for keywords like 'festival', 'event', 'directory' in the URL.
      - **CCR:** C:2, C:10, R:2
      - **Verification:** The function correctly classifies URLs based on predefined patterns.
    - **[ ] 3.2.2.2: Implement On-Page Keyword Analysis**
      - **Guidance:** Add logic to extract and analyze keywords from `<h1>`, `<h2>`, and `<title>` tags.
      - **CCR:** C:3, C:9, R:3
      - **Verification:** The function correctly classifies pages based on on-page keywords.
    - **[ ] 3.2.2.3: Implement Gemini API Fallback**
      - **Guidance:** Integrate the Gemini API call as a fallback for when the heuristic methods are not confident.
      - **CCR:** C:4, C:9, R:5
      - **Verification:** The Gemini API is called when needed and returns the correct classification.
  - **[ ] 3.2.3: Create Schema for Events and Directories**
    - **Guidance:** Update the Supabase schema to include tables for `events` and `source_directories`.
    - **CCR:** C:4, C:9, R:4 -> **Action:** Break down.
    - **Verification:** The new tables exist in Supabase.
    - **[ ] 3.2.3.1: Draft SQL for New Tables**
      - **Guidance:** Write the `CREATE TABLE` statements for the `events` and `source_directories` tables.
      - **CCR:** C:2, C:10, R:2
      - **Verification:** The SQL is syntactically correct.
    - **[ ] 3.2.3.2: Apply Schema Changes via Migration**
      - **Guidance:** Create a new Supabase migration file with the SQL for the new tables and apply it.
      - **CCR:** C:2, C:10, R:3
      - **Verification:** The new tables are visible in the Supabase dashboard.

- **[ ] 3.3: Implement Geocoding for Addresses**
  - **Guidance:** Some trucks will only have a physical address. We need a process to convert these addresses into latitude and longitude to display them on the map.
  - **CCR:** C:7, C:8, R:6 -> **Action:** Break down.
  - **Common Pitfalls & Fallbacks:**
    - **Inaccurate Geocoding:** The PostGIS geocoder may fail on poorly formatted or ambiguous addresses. **Fallback:** If the PostGIS trigger fails to find coordinates, the `pipelineProcessor.ts` script will make a follow-up call to the Nominatim API, which is better at handling fuzzy searches.
    - **Rate Limiting:** The Nominatim API has a strict usage policy (1 request per second). **Fallback:** Implement a queueing system with a delay for address lookups to avoid being blocked.
    - **No Results:** Neither service may find a given address. **Fallback:** Flag the truck in the admin dashboard for manual review and location entry.
    - **Debugging:** Two trucks need coordinate validation despite having addresses.
  - **Impact Analysis:**
    - **Associated Files:** `supabase/migrations/<timestamp>_enable_postgis.sql`, `supabase/migrations/<timestamp>_create_geocoding_trigger.sql`, `lib/pipelineProcessor.ts`, `docs/DATA_PIPELINE_ARCHITECTURE.md`
    - **Affected Files:** `food_trucks` table (performance), `components/map/MapComponent.tsx`, external dependency on Nominatim.
  - **Verification:** Trucks with only an address are visible on the map in the correct location.
  - **[x] 3.3.1: Research Geocoding Services**
    - **Guidance:** Evaluate free and open-source geocoding services. Options could include Nominatim (OpenStreetMap), or a free tier of a commercial service. Check Supabase for built-in PostGIS functions that might help.
    - **Tools/Libraries:** Supabase PostGIS extensions (`postgis`, `postgis_tiger_geocoder`), Nominatim API (for fallback).
    - **CCR:** C:3, C:9, R:3
    - **Verification:** A service is chosen and documented in `docs/DATA_PIPELINE_ARCHITECTURE.md`.
- **[ ] 3.3.2: Integrate Geocoding into Data Pipeline**
  - **Guidance:** Add a step in the pipeline that checks if GPS coordinates are missing and, if so, calls the chosen geocoding service to populate them.
  - **CCR:** C:7, C:8, R:6 -> **Action:** Break down.
  - **Verification:** The `latitude` and `longitude` columns in the `trucks` table are populated for address-only entries after a scrape.
  - **Future Changes:** Validate and possibly fix coordinates for existing trucks with addresses. Ensure correct usage of coordinates for map display.
    - **[ ] 3.3.2.1: Enable PostGIS Extensions in Supabase**
      - **Guidance:** Run the `CREATE EXTENSION` commands for `postgis` and `postgis_tiger_geocoder` in the Supabase SQL Editor.
      - **CCR:** C:2, C:10, R:3
      - **Verification:** The extensions are successfully enabled.
    - **[ ] 3.3.2.2: Create Geocoding Trigger Function**
      - **Guidance:** Write and apply the SQL for the trigger function that will automatically geocode addresses on `INSERT` or `UPDATE`.
      - **CCR:** C:4, C:9, R:5
      - **Verification:** The function is created in Supabase.
    - **[ ] 3.3.2.3: Attach Trigger to `food_trucks` Table**
      - **Guidance:** Write and apply the `CREATE TRIGGER` SQL statement.
      - **CCR:** C:2, C:10, R:4
      - **Verification:** The trigger is attached to the table and fires on `INSERT` or `UPDATE`.
    - **[ ] 3.3.2.4: Implement Nominatim Fallback Logic**
      - **Guidance:** In `lib/pipelineProcessor.ts`, add the logic to call the Nominatim API if the database geocoding fails (i.e., `latitude` is still null after insert/update).
      - **CCR:** C:4, C:9, R:5
      - **Verification:** The fallback logic is implemented and correctly updates the database.
  - **[✅] 3.3.3: Implement Client-Side Geocoding for Map Display (DEPRECATED)**
    - **Guidance:** ~~To immediately address the map display issue where only one food truck appears despite trucks having addresses, implement client-side geocoding within the MapComponent to convert addresses to coordinates on-the-fly.~~ **DEPRECATED: Replaced with immediate display approach due to poor UX.**
    - **CCR:** C:7, C:8, R:6 -> **Action:** COMPLETED, THEN REFACTORED
    - **Verification:** All food trucks with valid addresses now appear on the map with proper pins.
    - **[x] 3.3.3.1: Create Geocoding Utility Module**
      - **Guidance:** Create `lib/utils/geocoding.ts` using the free Nominatim API from OpenStreetMap to convert addresses to coordinates with rate limiting and proper error handling.
      - **CCR:** C:3, C:9, R:4
      - **Verification:** The utility module is created with proper error handling and rate limiting.
    - **[x] 3.3.3.2: Update MapComponent with Geocoding Logic (REFACTORED)**
      - **Guidance:** ~~Modify `components/map/MapComponent.tsx` to: Detect trucks with valid addresses but missing/default coordinates, Call the geocoding API asynchronously for those trucks, Merge geocoded coordinates back into truck data, Display all trucks with coordinates on the map~~ **REFACTORED: Removed client-side geocoding for immediate display with fallback coordinates.**
      - **CCR:** C:4, C:8, R:5
      - **Verification:** The MapComponent now displays all trucks immediately without geocoding delays or "Geocoding truck locations..." loading messages.
    - **[x] 3.3.3.3: Implement Coordinate Validation Logic (SIMPLIFIED)**
      - **Guidance:** ~~Add logic to distinguish between meaningful coordinates and Charleston fallback coordinates (32.7767, -79.9311) to ensure proper geocoding of trucks that only have default coordinates.~~ **SIMPLIFIED: Now uses simple fallback logic for immediate display.**
      - **CCR:** C:2, C:9, R:3
      - **Verification:** Trucks display immediately using existing coordinates or Charleston fallback for missing coordinates.
    - **[ ] 3.3.3.4: Validate and Fix Existing Truck Coordinates**
      - **Guidance:** Review and fix coordinates for all existing trucks to ensure unique pin locations when possible. Some trucks may have duplicate default coordinates (32.7767, -79.9311) or inaccurate coordinates that need to be corrected using their actual addresses.
      - **CCR:** C:6, C:8, R:5 -> **Action:** Break down.
      - **Impact Analysis:**
        - **Associated Files:** `supabase/migrations/<timestamp>_fix_truck_coordinates.sql`, `scripts/coordinate-validation.ts`, `lib/utils/geocoding.ts`
        - **Affected Files:** `food_trucks` table, MapComponent display, admin dashboard coordinate validation
      - **Verification:** All trucks with valid addresses have unique, accurate coordinates and display at correct locations on the map.
      - **[ ] 3.3.3.4.1: Research Coordinate Fixing Best Practices**
        - **Guidance:** Research examples and best practices for bulk coordinate validation and fixing in geocoding applications. Look for strategies to:
          - Identify and prioritize trucks with duplicate/default coordinates
          - Validate accuracy of existing coordinates against addresses
          - Handle edge cases (PO boxes, mobile trucks, etc.)
          - Minimize API calls while maximizing accuracy
        - **CCR:** C:3, C:9, R:3
        - **Verification:** Best practices and approach documented in `docs/DATA_PIPELINE_ARCHITECTURE.md`.
      - **[ ] 3.3.3.4.2: Audit Current Truck Coordinates**
        - **Guidance:** Query the database to identify trucks that:
          - Have default Charleston coordinates (32.7767, -79.9311) but valid addresses
          - Have duplicate coordinates (multiple trucks at same location)
          - Have coordinates that seem inconsistent with their listed address
          - Missing coordinates entirely
        - **CCR:** C:3, C:10, R:4
        - **Verification:** A comprehensive audit report identifying problematic coordinates is created.
      - **[ ] 3.3.3.4.3: Create Coordinate Validation Script**
        - **Guidance:** Create a Node.js script that can:
          - Connect to Supabase database
          - Iterate through trucks with problematic coordinates
          - Use geocoding API to get accurate coordinates from addresses
          - Update database with corrected coordinates
          - Log all changes for review
        - **CCR:** C:5, C:8, R:5
        - **Verification:** The script successfully validates and updates coordinates with proper error handling.
      - **[ ] 3.3.3.4.4: Execute Coordinate Fixes**
        - **Guidance:** Run the validation script against the production database, starting with trucks that have the most obvious issues (default coordinates with valid addresses).
        - **CCR:** C:3, C:9, R:4
        - **Verification:** Updated coordinates are accurate and trucks appear in correct locations on the map.
      - **[ ] 3.3.3.4.5: Implement Ongoing Coordinate Monitoring**
        - **Guidance:** Add checks to the data quality scoring system to flag trucks with potentially inaccurate coordinates and integrate coordinate validation into the admin dashboard.
        - **CCR:** C:4, C:8, R:4
        - **Verification:** The admin dashboard shows coordinate quality metrics and flags problematic entries.

- **[ ] 3.4: Define and Refine Data Quality Score**
  - **Guidance:** The current quality score is ambiguous. We need to define the specific metrics, weights, and thresholds that contribute to the score. This score should also inform a user-friendly "onboarding progress" metric for food truck owners.
  - **CCR:** C:6, C:5, R:5 -> **Action:** Break down.
  - **Impact Analysis:**
    - **Associated Files:** `lib/utils/QualityScorer.ts`, `docs/DATA_QUALITY_GUIDE.md`, `app/api/cron/quality-check/route.ts`
    - **Affected Files:** `food_trucks` table (`quality_score` column), `app/admin/data-quality/page.tsx` and its components, future food truck owner portal.
  - **Verification:** The logic for calculating the quality score is documented and implemented, and the scores in the admin dashboard reflect the new, clear metrics.
  - **[x] 3.4.1: Research Best Practices for Data Quality Metrics**
    - **Guidance:** Look at how other data-driven applications quantify data quality. Identify key fields for a food truck (e.g., has menu, has recent location, has photo, has operating hours) and assign weights to them.
    - **CCR:** C:3, C:9, R:2
    - **Verification:** A formula and a list of weighted metrics are documented in `docs/DATA_QUALITY_GUIDE.md`.
  - **[ ] 3.4.2: Implement New Quality Score Logic**
    - **Guidance:** Update the backend script that calculates the quality scores based on the new formula.
    - **CCR:** C:5, C:9, R:5 -> **Action:** Break down.
    - **Verification:** The scores are recalculated and updated in the database.
    - **[ ] 3.4.2.1: Refactor `DataQualityService`**
      - **Guidance:** Update the `calculateQualityScore` method in `lib/utils/QualityScorer.ts` to use the new weighted system from `docs/DATA_QUALITY_GUIDE.md`.
      - **CCR:** C:3, C:10, R:4
      - **Verification:** The method correctly calculates scores based on the new logic.
    - **[ ] 3.4.2.2: Update `quality-check` CRON Job**
      - **Guidance:** Ensure the `quality-check` CRON job correctly calls the updated `DataQualityService` and stores the new scores.
      - **CCR:** C:2, C:10, R:3
      - **Verification:** The CRON job runs successfully and updates the `quality_score` column in the `food_trucks` table.

- **[ ] 3.5: Verification Checkpoint: Data Pipeline**
  - **Guidance:** After completing all tasks in section 3.0, perform a full verification cycle.
  - **CCR:** C:2, C:10, R:4
  - **Verification:**
    - **[ ] 3.5.1: Run `npx tsc --noEmit`** - Confirm no new TypeScript errors.
    - **[ ] 3.5.2: Run `npx eslint .`** - Confirm no new linting errors.
    - **[ ] 3.5.3: Run `npx jscpd .`** - Confirm no new code duplication.
    - **[ ] 3.5.4: Manual E2E Test** - Manually trigger a scrape and verify that a) the CRON job runs, b) entities are classified correctly, and c) addresses are geocoded.

---

## 4.0 UI/UX Overhaul & Frontend Features

- **[✅] 4.1: Redesign Core UI Components**
  - **Guidance:** The current UI is described as "boxy" and "square". The goal is to create a more modern, rounded, and visually appealing design, based on a "black glassmorphism with red neon highlights" theme.
  - **CCR:** C:7, C:7, R:5 -> **Action:** COMPLETED.
  - **Common Pitfalls & Fallbacks:**
    - **Styling Conflicts:** A new UI library could conflict with existing Tailwind CSS classes. **Fallback:** Use a dedicated Tailwind CSS plugin like `tailwindcss-bg-glass` for the glassmorphism effect to minimize conflicts.
    - **Performance:** Glassmorphism and other complex styles can impact rendering performance. **Fallback:** Use these effects sparingly on key components and test performance with Lighthouse.
    - **Browser Incompatibility:** The `backdrop-filter` property used for glassmorphism is not supported in all browsers. **Fallback:** Provide a solid color fallback for unsupported browsers.
  - **Impact Analysis:**
    - **Associated Files:** `app/globals.css`, `tailwind.config.ts`, `components/ui/card.tsx`, `components/ui/button.tsx`, `package.json`, `docs/UI_DESIGN_GUIDE.md`
    - **Affected Files:** Potentially all components in the application due to global style changes; snapshot tests for UI components.
  - **Verification:** The application's UI reflects the new design language.
  - **[x] 4.1.1: Research UI Frameworks and Design Systems**
    - **Guidance:** Investigate "Magic UI" and other open-source component libraries/frameworks that can help achieve the desired aesthetic.
    - **Tools/Libraries:** `tailwindcss-bg-glass`, `framer-motion` (for animations).
    - **CCR:** C:3, C:9, R:2
    - **Verification:** A decision on whether to use a library or build custom styles is made and documented in `docs/UI_DESIGN_GUIDE.md`.
  - **[✅] 4.1.2: Update Base Styles and Component Library**
    - **Guidance:** Modify `app/globals.css`, `tailwind.config.ts`, and core UI components in `components/ui/` to use more `border-radius`, new color schemes, and other modern styling.
    - **CCR:** C:6, C:9, R:5 -> **Action:** COMPLETED.
    - **Verification:** Components like Cards, Buttons, and Badges have a new, rounded look.
    - **[x] 4.1.2.1: Implement Custom Glassmorphism Solution**
      - **Guidance:** Instead of external library, implemented custom glassmorphism using Tailwind CSS utilities and CSS variables in `globals.css`.
      - **CCR:** C:2, C:10, R:3
      - **Verification:** Custom `.glass`, `.glass-strong`, `.neon-border`, and `.neon-glow` utilities are implemented.
    - **[x] 4.1.2.2: Refactor `Card` Component**
      - **Guidance:** Updated the `Card` component to use the `glass` class and `rounded-modern` styling.
      - **CCR:** C:2, C:10, R:2
      - **Verification:** The `Card` component now has a glassmorphism effect with modern rounded corners.
    - **[x] 4.1.2.3: Refactor `Button` Component**
      - **Guidance:** Updated the `Button` component using `tailwind-variants` with neon glow effects, hover animations, and scale transforms.
      - **CCR:** C:3, C:9, R:3
      - **Verification:** The `Button` component has multiple variants including `neon`, `ghost`, and `secondary` with desired effects.
    - **[x] 4.1.2.4: Create Component Variants System**
      - **Guidance:** Implemented `components/ui/variants.ts` using `tailwind-variants` for consistent styling across all UI components.
      - **CCR:** C:4, C:9, R:4
      - **Verification:** Comprehensive variant system includes buttons, badges, alerts, sheets, and other components.

  - **[✅] 4.2: Improve "View Details" Experience**
    - **Guidance:** Clicking "View Details" should not navigate to a new page. It should display the information on the same page, using a modal, accordion, or similar non-disruptive UI pattern. For desktop users, implement a sliding panel from below the map to allow simultaneous viewing of truck location and details.
    - **CCR:** C:6, C:9, R:4 -> **Action:** MOBILE COMPLETED, DESKTOP ENHANCEMENT NEEDED.
    - **Impact Analysis:**
      - **Associated Files:** `components/TruckCard.tsx`, `components/home/TruckListSection.tsx`, `components/ui/AlertDialog.tsx`, `app/trucks/[id]/page.tsx`
      - **Affected Files:** Core user experience, application routing and state management, styling of the new detail view.
    - **Verification:** The user can view truck details without a full page load, and desktop users can see both map location and details simultaneously.
    - **[x] 4.2.1: Implement Non-Navigational Detail View (Mobile)**
      - **Guidance:** Refactor the `TruckCard` and `TruckListSection` components to open details in a modal (`AlertDialog` or `Sheet` from `components/ui`) or by expanding the card itself.
      - **CCR:** C:5, C:9, R:4
      - **Verification:** The new detail view is functional and smooth.
    - **[x] 4.2.1.1: Create Detail View Modal**
      - **Guidance:** Create a new component for the detail view modal using the `AlertDialog` component.
      - **CCR:** C:3, C:10, R:2
      - **Verification:** The modal component is created and can be triggered.
    - **[x] 4.2.1.2: Integrate Modal into `TruckCard`**
      - **Guidance:** Add the logic to the `TruckCard` to open the modal on "View Details" click.
      - **CCR:** C:3, C:9, R:3
      - **Verification:** The modal opens with the correct truck's details.
    - **[x] 4.2.2: Hide Developer-Facing Data**
      - **Guidance:** Remove fields like "Coordinates" from the user-facing detail view. This data is useful for admins but not for regular users.
      - **CCR:** C:1, C:10, R:1
      - **Verification:** The detail view only shows user-friendly information.
    - **[ ] 4.2.3: Implement Desktop Sliding Panel**
      - **Guidance:** For desktop users (screen width >= 1024px), replace the modal with a sliding panel that emerges from below the map. This allows users to see both the truck's location on the map and its details simultaneously, improving the user experience by eliminating the choice between location vs details.
      - **CCR:** C:5, C:8, R:4
      - **Impact Analysis:**
        - **Associated Files:** `components/TruckDetailsModal.tsx`, `components/ui/sheet.tsx`, responsive design utilities
        - **Affected Files:** Main page layout, map container sizing, TruckCard click handlers
      - **Verification:** On desktop, clicking "View Details" opens a sliding panel below the map while keeping the map visible and interactive.
      - **[ ] 4.2.3.1: Create Responsive Detail View Logic**
        - **Guidance:** Implement logic to detect screen size and choose between modal (mobile) and sliding panel (desktop) presentation.
        - **CCR:** C:2, C:10, R:3
        - **Verification:** The component correctly chooses the appropriate presentation method based on screen size.
      - **[ ] 4.2.3.2: Implement Desktop Sliding Panel Component**
        - **Guidance:** Create a sliding panel component that animates from below the map area, using the existing glassmorphism design language.
        - **CCR:** C:4, C:9, R:4
        - **Verification:** The sliding panel animates smoothly and maintains the application's design consistency.
      - **[ ] 4.2.3.3: Update Layout for Panel Integration**
        - **Guidance:** Modify the main page layout to accommodate the sliding panel without disrupting the map or truck list sections.
        - **CCR:** C:3, C:9, R:3
        - **Verification:** The layout gracefully handles the sliding panel on desktop while maintaining mobile compatibility.

  - **[✅] 4.3: Replace Food Truck Icon**
    - **Guidance:** The current map marker for food trucks needs to be replaced with a better icon that clearly represents a food truck.
    - **CCR:** C:2, C:10, R:1
    - **Impact Analysis:**
      - **Associated Files:** `public/new-icon.svg`, `components/map/MapComponent.tsx`
      - **Affected Files:** Map marker visuals, Leaflet icon configuration.
    - **Verification:** The map displays the new, improved food truck icon.
    - **[x] 4.3.1: Source a New Food Truck Icon**
      - **Guidance:** Find a suitable, open-source (or free-to-use) SVG icon for a food truck.
      - **CCR:** C:1, C:10, R:0
      - **Verification:** A new SVG file is added to the `public/` directory.
    - **[x] 4.3.2: Update Map Component to Use New Icon**
      - **Guidance:** Modify `components/map/MapComponent.tsx` to use the new icon for markers.
      - **CCR:** C:2, C:10, R:2
      - **Verification:** The new icon appears correctly on the map.

- **[ ] 4.4: Design and Implement User Portal**
  - **Guidance:** Create a user-specific area, accessed via a sliding pop-out hamburger menu or user icon in the header. This will be the foundation for personalized features.
  - **CCR:** C:8, C:6, R:7 -> **Action:** Break down.
  - **Impact Analysis:**
    - **Associated Files:** `docs/USER_PORTAL_PLAN.md`, `components/TruckCard.tsx`, `supabase/migrations/<timestamp>_create_user_favorites.sql`, new portal components (`components/user/*`), new route (`app/user/*`)
    - **Affected Files:** Database schema, application UI, API layer, authentication system, client-side state management.
  - **Verification:** A basic user portal is accessible after login.
  - **[ ] 4.4.1: Plan User Portal Features (Phase 1)**
    - **Guidance:** For the initial version, plan for a minimal set of features: viewing favorited trucks and a settings page. Document plans for future features like notifications and coupons.
    - **CCR:** C:3, C:9, R:2
    - **Verification:** A feature list for the user portal is documented in `docs/USER_PORTAL_PLAN.md`.
  - **[ ] 4.4.2: Implement User Favorites**
    - **Guidance:** Add a "favorite" button to `TruckCard`. Create a new table in Supabase to store user favorites (`user_id`, `truck_id`). Display favorited trucks in the user portal.
    - **CCR:** C:7, C:8, R:6 - **Action:** PARTIALLY COMPLETED.
    - **Verification:** Users can favorite and unfavorite trucks, and the list appears in their portal.
    - **[✅] 4.4.2.1: Create `user_favorites` Table**
      - **Guidance:** Write and apply a Supabase migration to create the `user_favorites` table with `user_id` and `truck_id` columns.
      - **CCR:** C:2, C:10, R:3
      - **Verification:** The table is created in Supabase.
    - **[✅] 4.4.2.1.1: Implement RLS Policies for `user_favorites`**
      - **Guidance:** After creating the table, immediately enable RLS and apply policies that ensure users can only access their own favorite entries. The policies should check that `auth.uid() = user_id`.
      - **CCR:** C:3, C:10, R:4
      - **Verification:** RLS policies are active on the table, and tests confirm that one user cannot access another user's favorites.
    - **[ ] 4.4.2.2: Implement "Favorite" Button UI**
      - **Guidance:** Add a "favorite" button to the `TruckCard` component, with logic to handle the visual state (favorited/not favorited).
      - **CCR:** C:3, C:10, R:2
      - **Verification:** The button appears and changes state on click.
    - **[ ] 4.4.2.3: Implement Favorite/Unfavorite Logic**
      - **Guidance:** Create the client-side and server-side logic to add/remove entries from the `user_favorites` table when the button is clicked.
      - **CCR:** C:4, C:9, R:4
      - **Verification:** Clicking the button correctly updates the database.
    - **[ ] 4.4.2.4: Display Favorites in User Portal**
      - **Guidance:** Create a new component to fetch and display the user's favorited trucks in their portal.
      - **CCR:** C:3, C:10, R:3
      - **Verification:** The user's favorites are correctly displayed.

- **[ ] 4.5: Verification Checkpoint: UI/UX**
  - **Guidance:** After completing all tasks in section 4.0, perform a full verification cycle.
  - **CCR:** C:2, C:10, R:4
  - **Verification:**
    - **[ ] 4.5.1: Run `npx tsc --noEmit`** - Confirm no new TypeScript errors.
    - **[ ] 4.5.2: Run `npx eslint .`** - Confirm no new linting errors.
    - **[ ] 4.5.3: Run `npx jscpd .`** - Confirm no new code duplication.
    - **[ ] 4.5.4: Manual E2E Test** - Manually test the new UI components, the "View Details" modal, and the user favorites functionality.

---

## 5.0 Admin Dashboard Enhancements

- **[ ] 5.1: Fix Data Discrepancies**
  - **Guidance:** The admin dashboard panels are not all showing the same data (e.g., 9 trucks vs. 10 trucks). This suggests a data fetching or state management issue.
  - **CCR:** C:7, C:6, R:7 -> **Action:** Break down.
  - **Impact Analysis:**
    - **Associated Files:** `components/admin/dashboard/TrucksPage.tsx`, `app/admin/data-quality/page.tsx`, `package.json`, `lib/queryClient.ts` (new)
    - **Affected Files:** All data-fetching components in the admin dashboard, application's data fetching and caching strategy.
  - **Verification:** All panels on the admin dashboard show consistent data that matches the database.
  - **[ ] 5.1.1: Investigate Data Fetching Logic**
    - **Guidance:** Review the data fetching code for each panel on the admin dashboard (`app/admin/page.tsx` and its components). Check if some panels are using cached data or different query filters.
    - **CCR:** C:4, C:9, R:4 -> **Action:** Break down.
    - **Verification:** The root cause of the discrepancy is identified and documented.
    - **[ ] 5.1.1.1: Analyze `TrucksPage` Component**
      - **Guidance:** Examine the data fetching in `components/admin/dashboard/TrucksPage.tsx`.
      - **CCR:** C:2, C:10, R:2
      - **Verification:** The data fetching logic is understood and documented.
    - **[ ] 5.1.1.2: Analyze Data Quality Components**
      - **Guidance:** Examine the data fetching in `app/admin/data-quality/page.tsx` and its child components.
      - **CCR:** C:2, C:10, R:2
      - **Verification:** The data fetching logic is understood and documented.
  - **[ ] 5.1.2: Unify Data Source or Refetching Strategy**
    - **Guidance:** Refactor the dashboard to use a single, consistent data source (e.g., a React context or a unified hook) or ensure all panels refetch data on a consistent trigger.
    - **CCR:** C:7, C:8, R:7 -> **Action:** Break down.
    - **Verification:** The fix is implemented and all panels update in sync.
    - **[ ] 5.1.2.1: Implement TanStack Query**
      - **Guidance:** Integrate TanStack Query (React Query) for server state management to ensure data is fetched and cached consistently across all components.
      - **CCR:** C:5, C:8, R:6 -> **Action:** Break down.
      - **Verification:** TanStack Query is set up and used for data fetching in the admin dashboard.
    - **[ ] 5.1.2.1.1: Install and Configure TanStack Query**
      - **Guidance:** Add the `@tanstack/react-query` package and set up the `QueryClientProvider`.
      - **CCR:** C:2, C:10, R:3
      - **Verification:** TanStack Query is installed and configured.
    - **[ ] 5.1.2.1.2: Create a `useTrucks` Hook**
      - **Guidance:** Create a custom hook that uses `useQuery` to fetch the list of trucks.
      - **CCR:** C:3, C:10, R:4
      - **Verification:** The hook is created and successfully fetches truck data.
    - **[ ] 5.1.2.1.3: Refactor Components to Use `useQuery`**
      - **Guidance:** Replace the existing `useEffect`-based data fetching with the `useQuery` hook from TanStack Query in all admin components.
      - **CCR:** C:3, C:9, R:4
      - **Verification:** All admin dashboard components use `useQuery` for data fetching.

- **[ ] 5.2: Improve Auto-Scraping Page**
  - **Guidance:** The auto-scraping page is vague. It needs to provide more useful information about scraping status and history.
  - **CCR:** C:6, C:7, R:5 -> **Action:** Break down.
  - **Verification:** The auto-scraping page is more informative and functional.
  - **[ ] 5.2.1: Display Scraping History**
    - **Guidance:** Create a table to log the status of each scraping batch job (start time, end time, status, number of trucks processed). Display this history on the auto-scraping page.
    - **CCR:** C:5, C:9, R:5 -> **Action:** Break down.
    - **Verification:** The page shows a history of past scraping jobs.
    - **[ ] 5.2.1.1: Create `scraping_history` Table**
      - **Guidance:** Write and apply a Supabase migration to create the `scraping_history` table.
      - **CCR:** C:2, C:10, R:3
      - **Verification:** The table is created in Supabase.
    - **[ ] 5.2.1.2: Log Scrapes to History Table**
      - **Guidance:** Modify the `auto-scrape` CRON job to log the results of each run to the new table.
      - **CCR:** C:3, C:9, R:4
      - **Verification:** The CRON job correctly logs its status.
    - **[ ] 5.2.1.3: Create History Table Component**
      - **Guidance:** Create a new React component to fetch and display the scraping history.
      - **CCR:** C:3, C:10, R:2
      - **Verification:** The component is created and displays the history.
  - **[ ] 5.2.2: Implement "Trigger Manual Scrape" Functionality**
    - **Guidance:** The button currently does nothing. Hook it up to an API endpoint that triggers the scraping pipeline on demand. Provide feedback to the user that the scrape has started.
    - **CCR:** C:5, C:9, R:4 -> **Action:** Break down.
    - **Verification:** Clicking the button successfully initiates a new scraping job.
    - **[ ] 5.2.2.1: Create Manual Scrape API Endpoint**
      - **Guidance:** Create a new API route that triggers the `autoScraper.runAutoScraping()` function.
      - **CCR:** C:3, C:10, R:4
      - **Verification:** The API endpoint is created and can be called.
    - **[ ] 5.2.2.2: Connect Button to Endpoint**
      - **Guidance:** Add the client-side logic to call the new API endpoint when the button is clicked.
      - **CCR:** C:2, C:10, R:3
      - **Verification:** The button successfully triggers the API endpoint.

- **[ ] 5.3: Improve Data Quality Page UI**
  - **Guidance:** The charts on the Data Quality page are boxy and the pie chart labels overlap.
  - **CCR:** C:5, C:8, R:3 -> **Action:** Break down.
  - **Verification:** The charts are visually appealing and the labels are readable.
  - **[ ] 5.3.1: Fix Pie Chart Labels**
    - **Guidance:** Adjust the configuration of the charting library (likely Recharts) to position the labels correctly, perhaps below the chart with a legend.
    - **CCR:** C:3, C:10, R:2
    - **Verification:** The pie chart is legible.
  - **[ ] 5.3.2: Apply New Design to Charts**
    - **Guidance:** Update the chart components to match the new, rounded design language.
    - **CCR:** C:3, C:9, R:3
    - **Verification:** The charts have rounded corners and fit the new aesthetic.

- **[ ] 5.4: Implement Smoother Page Refresh**
  - **Guidance:** The admin dashboard appears to be auto-refreshing periodically in a jarring way. Investigate the cause and replace it with a smoother, less obvious data-refetching mechanism.
  - **CCR:** C:5, C:7, R:4 -> **Action:** Break down.
  - **Verification:** The page no longer performs a full refresh; data updates smoothly in the background.
  - **[ ] 5.4.1: Identify Source of Auto-Refresh**
    - **Guidance:** Investigate the codebase, particularly in the admin layout and page files, for any `setInterval` or meta-refresh tags that could be causing the refresh.
    - **CCR:** C:2, C:10, R:2
    - **Verification:** The code causing the refresh is identified.
  - **[ ] 5.4.2: Replace Refresh with TanStack Query `refetchInterval`**
    - **Guidance:** Remove the old refresh logic. In the `useQuery` hooks implemented in task 5.1.2, use the `refetchInterval` option to poll for new data in the background without a full page reload.
    - **CCR:** C:3, C:9, R:3
    - **Verification:** The old refresh logic is removed, and data now updates in the background.

- **[ ] 5.5: Verification Checkpoint: Admin Dashboard**
  - **Guidance:** After completing all tasks in section 5.0, perform a full verification cycle.
  - **CCR:** C:2, C:10, R:4
  - **Verification:**
    - **[ ] 5.5.1: Run `npx tsc --noEmit`** - Confirm no new TypeScript errors.
    - **[ ] 5.5.2: Run `npx eslint .`** - Confirm no new linting errors.
    - **[ ] 5.5.3: Run `npx jscpd .`** - Confirm no new code duplication.
    - **[ ] 5.5.4: Manual E2E Test** - Manually test the admin dashboard, confirming data is consistent, scraping history appears, manual scraping works, and charts are fixed.

---

## 6.0 Future-Proofing & Deployment

- **[ ] 6.1: Plan for Mobile App Conversion**
  - **Guidance:** Research methods for wrapping the Next.js web app into a mobile app for the Google Play Store, prioritizing cost-effective solutions.
  - **CCR:** C:7, C:5, R:6 -> **Action:** Break down.
  - **Verification:** A document, `docs/MOBILE_DEPLOYMENT_PLAN.md`, is created, comparing technologies like Progressive Web Apps (PWA), Capacitor, and others.
    - **[ ] 6.1.1: Research PWA Conversion**
      - **Guidance:** Investigate the steps required to make the Next.js app a fully-featured Progressive Web App.
      - **CCR:** C:3, C:9, R:3
      - **Verification:** The PWA conversion process is documented.
    - **[ ] 6.1.2: Research Capacitor Conversion**
      - **Guidance:** Investigate the steps required to wrap the app with Capacitor.
      - **CCR:** C:3, C:9, R:4
      - **Verification:** The Capacitor conversion process is documented.
- **[ ] 6.2: Plan for Payment Integration**
  - **Guidance:** Research and plan for the integration of a payment provider for food truck owner subscriptions. The preference is for Polar over Stripe.
  - **CCR:** C:6, C:6, R:8 -> **Action:** Break down.
  - **Verification:** A document, `docs/PAYMENT_INTEGRATION_PLAN.md`, is created, outlining the chosen provider, the required data schema changes, and the implementation steps.
    - **[ ] 6.2.1: Research Polar Payments**
      - **Guidance:** Investigate the Polar Payments API, pricing, and integration with Next.js.
      - **CCR:** C:3, C:9, R:4
      - **Verification:** The research is documented in `docs/PAYMENT_INTEGRATION_PLAN.md`.
    - **[ ] 6.2.2: Design Subscription Schema**
      - **Guidance:** Design the necessary Supabase tables to manage user subscriptions, plans, and payment statuses.
      - **CCR:** C:3, C:9, R:4
      - **Verification:** The schema is documented in `docs/PAYMENT_INTEGRATION_PLAN.md`.

- **[ ] 6.3: Plan for Analytics Dashboard**
  - **Guidance:** Plan the analytics page for monitoring token usage and costs for services like Firecrawl, Tavily, and Gemini.
  - **CCR:** C:5, C:6, R:4 -> **Action:** Break down.
  - **Verification:** A new document, `docs/ANALYTICS_DASHBOARD_PLAN.md`, is created with a schema design and UI mockups.
    - **[ ] 6.3.1: Design `api_usage_metrics` Table**
      - **Guidance:** Design a Supabase table to log each API call, its service, cost, and associated user/job.
      - **CCR:** C:2, C:10, R:3
      - **Verification:** The table schema is documented.
    - **[ ] 6.3.2: Plan UI for Analytics**
      - **Guidance:** Sketch out the charts and tables needed to visualize API usage and costs over time.
      - **CCR:** C:2, C:10, R:2
      - **Verification:** The UI plan is documented.

---

## 7.0 Supabase Health & Optimization

- **[x] 7.1: Optimize RLS Policies**
  - **Guidance:** The database has multiple, overlapping permissive RLS policies for the same roles and actions, which hurts performance. These need to be consolidated. This is still critical even with an external auth provider.
  - **CCR:** C:7, C:6, R:7 -> **Action:** Break down.
  - **Verification:** The number of RLS policies is reduced, and queries on the affected tables show improved performance.
  - **[x] 7.1.1: Consolidate Multiple Permissive Policies**
    - **Guidance:** For tables like `events`, `food_truck_schedules`, and `menu_items`, combine the multiple `PERMISSIVE` policies for each action (`SELECT`, `INSERT`, `UPDATE`, `DELETE`) into a single, more efficient policy. This will involve creating more comprehensive `USING` and `WITH CHECK` expressions based on the user's role derived from their JWT.
    - **CCR:** C:4, C:9, R:5
    - **Verification:** The redundant policies are removed from the Supabase dashboard, leaving one policy per action/role.

- **[x] 7.2: Optimize Database Indexing**
  - **Guidance:** Address the indexing warnings from the Supabase Advisor to improve query performance and remove dead weight.
  - **CCR:** C:6, C:7, R:6 -> **Action:** Break down.
  - **Verification:** The indexing-related warnings are no longer present in the Supabase Advisor report.
  - **[x] 7.2.1: Add Indexes to Foreign Keys**
    - **Guidance:** Add indexes to the foreign key columns identified in the advisor report (`data_processing_queue.truck_id`, `events.food_truck_id`, `food_truck_schedules.food_truck_id`). This can be done via the Supabase SQL editor.
    - **CCR:** C:2, C:10, R:3
    - **Verification:** The new indexes are visible in the Supabase table definitions.
  - **[ ] 7.2.2: Remove Unused Indexes**
    - **Guidance:** The advisor has identified a large number of unused indexes across various tables (`api_usage`, `discovered_directories`, `discovered_urls`, `food_trucks`, `scraping_jobs`). These should be reviewed and dropped to save space and reduce maintenance overhead.
    - **CCR:** C:5, C:8, R:5 -> **Action:** Break down.
    - **Verification:** The unused indexes are removed, and application functionality remains unaffected.
    - **[ ] 7.2.2.1: Analyze Index Usage**
      - **Guidance:** Use the `pg_stat_user_indexes` view in Supabase to confirm that the identified indexes have zero or very low usage.
      - **CCR:** C:2, C:10, R:2
      - **Verification:** The analysis confirms the advisor's findings.
    - **[ ] 7.2.2.2: Draft `DROP INDEX` Statements**
      - **Guidance:** Write the `DROP INDEX` statements for the confirmed unused indexes.
      - **CCR:** C:1, C:10, R:3
      - **Verification:** The SQL is syntactically correct.
    - **[ ] 7.2.2.3: Apply Index Deletions via Migration**
      - **Guidance:** Create a new Supabase migration file with the `DROP INDEX` statements and apply it.
      - **CCR:** C:2, C:10, R:4
      - **Verification:** The indexes are removed from the database.

- **[ ] 7.3: Verification Checkpoint: Supabase Health**
  - **Guidance:** After completing all tasks in section 7.0, perform a full verification cycle.
  - **CCR:** C:2, C:10, R:4
  - **Verification:**
    - **[ ] 7.3.1: Run `npx tsc --noEmit`** - Confirm no new TypeScript errors.
    - **[ ] 7.3.2: Run `npx eslint .`** - Confirm no new linting errors.
    - **[ ] 7.3.3: Run `npx jscpd .`** - Confirm no new code duplication.
    - **[ ] 7.3.4: Manual E2E Test** - Check the Supabase Advisor report to confirm that the RLS and indexing warnings have been resolved.
